{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de9449e",
   "metadata": {},
   "source": [
    "## 准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d18e7d",
   "metadata": {},
   "source": [
    "### 调用Qwen2.5-0.5B-Instruct\n",
    "\n",
    "使用 modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0cfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3b1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f71092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02e4470",
   "metadata": {},
   "source": [
    "### 调用Qwen3\n",
    "\n",
    "使用 transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"models/Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# # load the tokenizer and the model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # prepare the model input\n",
    "# prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ]\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True,\n",
    "#     enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# # conduct text completion\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=32768\n",
    "# )\n",
    "# output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# # parsing thinking content\n",
    "# try:\n",
    "#     # rindex finding 151668 (</think>)\n",
    "#     index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "# except ValueError:\n",
    "#     index = 0\n",
    "\n",
    "# thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "# content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# print(\"thinking content:\", thinking_content)\n",
    "# print(\"content:\", content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db7da0",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "\n",
    "AutoDL开启学术加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e016aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ca054",
   "metadata": {},
   "source": [
    "加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset('openai/gsm8k', 'main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c4ab9",
   "metadata": {},
   "source": [
    "### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "import wandb\n",
    "wandb.login(key=\"c948239ff3d1b179f0be4b63e3467397658b480a\")\n",
    "wandb.init(project=\"GRPO-test-0430\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc9e77",
   "metadata": {},
   "source": [
    "## GRPO训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc9f84",
   "metadata": {},
   "source": [
    "### 格式化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = load_dataset('openai/gsm8k', 'main')[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec860c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_gsm8k_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0236b1b",
   "metadata": {},
   "source": [
    "### ~~原始模型输出~~\n",
    "\n",
    "此时没有思考过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# model_name = \"models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "\n",
    "# messages = dataset[0]['prompt']\n",
    "\n",
    "# text = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# generated_ids = model.generate(\n",
    "#     **model_inputs,\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "# response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13050ac5",
   "metadata": {},
   "source": [
    "### 奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e95ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctness_reward_func：根据正确性对答案进行奖励。\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "# int_reward_func：根据是否为数字对输出进行奖励。\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "# strict_format_reward_func：根据严格的格式要求检查并奖励。\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# soft_format_reward_func：根据稍微宽松的格式要求检查并奖励。\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# count_xml：计算文本中的 XML 标签结构并给予奖励。\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "# xmlcount_reward_func：为每个输出计算 XML 标签结构的符合度并返回奖励。\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92ffab",
   "metadata": {},
   "source": [
    "### 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c939df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# dataset = dataset.select(range(300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"models/Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "output_dir=\"outputs/Qwen-0.5B-GRPO\"\n",
    "run_name=\"Qwen-0.5B-GRPO-gsm8k\" # 当前运行项目的名称\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4, # batchsize\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=16,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=200, # 回复的token数量\n",
    "    num_train_epochs=1,\n",
    "    save_steps=300,\n",
    "    save_total_limit=5, # 最多保留 5 个最新检查点，防止系统盘被占满\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    vllm_device=\"cuda:0\",\n",
    "    report_to=\"wandb\", # 报告工具\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43057533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89344d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17116e",
   "metadata": {},
   "source": [
    "### 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c57292",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_model_name = output_dir\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    grpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8599d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e99d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

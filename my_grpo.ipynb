{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de9449e",
   "metadata": {},
   "source": [
    "## 准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db7da0",
   "metadata": {},
   "source": [
    "### 数据集\n",
    "\n",
    "AutoDL开启学术加速，直接加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e016aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# data = load_dataset('openai/gsm8k', 'main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ca054",
   "metadata": {},
   "source": [
    "本地加载huggingface数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c712a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "base_url = \"data/gsm8k/main/\"\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': base_url + 'train-00000-of-00001.parquet', 'test': base_url + 'test-00000-of-00001.parquet'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c4ab9",
   "metadata": {},
   "source": [
    "### wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb5e26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb\n",
    "import wandb\n",
    "wandb.login(key=\"your_wandb_api_key\")  # 替换为你的wandb API密钥\n",
    "project_name = \"GRPO-Qwen2.5-3B-test\" # todo: wandb项目命名\n",
    "wandb.init(project = project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc9e77",
   "metadata": {},
   "source": [
    "## GRPO训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec9714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "XML_COT_FORMAT = \"\"\"\\\n",
    "<reasoning>\n",
    "{reasoning}\n",
    "</reasoning>\n",
    "<answer>\n",
    "{answer}\n",
    "</answer>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc9f84",
   "metadata": {},
   "source": [
    "### 格式化数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d95b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_xml_answer(text: str) -> str:\n",
    "    answer = text.split(\"<answer>\")[-1]\n",
    "    answer = answer.split(\"</answer>\")[0]\n",
    "    return answer.strip()\n",
    "\n",
    "def extract_hash_answer(text: str) -> str | None:\n",
    "    if \"####\" not in text:\n",
    "        return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "\n",
    "def get_gsm8k_questions(split = \"train\") -> Dataset:\n",
    "    data = dataset[split] # type: ignore\n",
    "    data = data.map(lambda x: { # type: ignore\n",
    "        'prompt': [\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': x['question']}\n",
    "        ],\n",
    "        'answer': extract_hash_answer(x['answer'])\n",
    "    }) # type: ignore\n",
    "    return data # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec860c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_gsm8k_questions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13050ac5",
   "metadata": {},
   "source": [
    "### 奖励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e95ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correctness_reward_func：根据正确性对答案进行奖励。\n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    q = prompts[0][-1]['content']\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n",
    "    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n",
    "\n",
    "# int_reward_func：根据是否为数字对输出进行奖励。\n",
    "def int_reward_func(completions, **kwargs) -> list[float]:\n",
    "    responses = [completion[0]['content'] for completion in completions]\n",
    "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
    "    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n",
    "\n",
    "# strict_format_reward_func：根据严格的格式要求检查并奖励。\n",
    "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# soft_format_reward_func：根据稍微宽松的格式要求检查并奖励。\n",
    "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [re.match(pattern, r) for r in responses]\n",
    "    return [0.5 if match else 0.0 for match in matches]\n",
    "\n",
    "# count_xml：计算文本中的 XML 标签结构并给予奖励。\n",
    "def count_xml(text) -> float:\n",
    "    count = 0.0\n",
    "    if text.count(\"<reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
    "        count += 0.125\n",
    "    if text.count(\"\\n<answer>\\n\") == 1:\n",
    "        count += 0.125\n",
    "        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n",
    "    if text.count(\"\\n</answer>\") == 1:\n",
    "        count += 0.125\n",
    "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n",
    "    return count\n",
    "\n",
    "# xmlcount_reward_func：为每个输出计算 XML 标签结构的符合度并返回奖励。\n",
    "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    return [count_xml(c) for c in contents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92ffab",
   "metadata": {},
   "source": [
    "### 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bfb03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: 控制数据集大小\n",
    "dataset = dataset.select(range(100))\n",
    "# todo: 选取模型\n",
    "model_name = \"Qwen2.5-3B-Instruct\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0746aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"models/\" + model_name\n",
    "output_dir = \"models/\" + model_name + \"-GRPO\"\n",
    "run_name = model_name + \"-GRPO-gsm8k\" # 当前运行项目的名称\n",
    "\n",
    "# todo: 超参数优化\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    run_name=run_name,\n",
    "    learning_rate=5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    bf16=True,\n",
    "    per_device_train_batch_size=4, # batchsize\n",
    "    gradient_accumulation_steps=4, # 梯度累积，使用梯度累积来模拟更大的批次大小\n",
    "    num_generations=4, # 生成的数量\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=256, # 回复的token数量\n",
    "    num_train_epochs=1,\n",
    "    save_steps=300,\n",
    "    save_total_limit=2, # 最多保留 5 个最新检查点，防止系统盘被占满\n",
    "    max_grad_norm=0.1,\n",
    "    log_on_each_node=False,\n",
    "    use_vllm=False,\n",
    "    vllm_gpu_memory_utilization=.3,\n",
    "    # vllm_device=\"cuda:0\",\n",
    "    report_to=\"wandb\", # 报告工具\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43057533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ").to(\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89344d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        xmlcount_reward_func,\n",
    "        soft_format_reward_func,\n",
    "        strict_format_reward_func,\n",
    "        int_reward_func,\n",
    "        correctness_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d17116e",
   "metadata": {},
   "source": [
    "### 查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c57292",
   "metadata": {},
   "outputs": [],
   "source": [
    "grpo_model_name = output_dir\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    grpo_model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8599d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Joy can read 8 pages of a book in 20 minutes. How many hours will it take her to read 120 pages?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e99d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
